<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/BlogPosting" >
  <link rel="stylesheet" type="text/css" href="/assets/css/screen.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Source+Code+Pro">
  <link rel="stylesheet" type="text/css" href="/assets/css/custom.css" />
  <link rel="stylesheet" type="text/css" href="/assets/css/grid.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/jupyter.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/notebook.css">
  <body>
    <section class="post">

    <article role="article" id="post" class="post-content" itemprop="articleBody">
    <p>Using Artificial Neural Networks to predict wine type and quality based on its chemical properties.</p>

<!--end-->

<h1 id="predicting-wine-artificial-neural-networks-with-keras">Predicting Wine: Artificial Neural Networks with Keras</h1>

<p><strong>Using Artificial Neural Networks to predict wine type and quality based on its chemical properties.</strong></p>

<h1 id="1-about-deep-learning">1. About Deep Learning</h1>
<p><strong>Deep Learning</strong></p>

<p>Deep Learning, a.k.a. Artificial Neural Networks (ANN), is a subfield of machine learning that is a set of algorithms that are inspired by the structure and function of the brain and is a popular fields in data science with many case studies involving robotics, image recognition and Artificial Intelligence (AI).</p>

<p>One of the most powerful Python libraries for developing and evaluating deep learning models is Keras; It wraps the efficient numerical computation libraries Theano and TensorFlow.</p>

<p><strong>Perceptrons</strong></p>

<p>The “perceptron” is a neural network, which, in its simplest form, consists of a single neuron. Much like biological neurons, that have dendrites and axons, the single artificial neuron is a simple tree structure that has input nodes and a single output node, and is connected to each input node.</p>

<p><strong>Here’s a visual comparison of the two:</strong><br />
<img src="/assets/media/wine/nueron-vs-ann.png" alt="NUERON-VS-ANN" /></p>

<p>There are six components to artificial neurons. From left to right, they are:</p>

<ol>
  <li>
    <p><code class="highlighter-rouge">Input nodes</code>. Each input node is associated with a numerical value, which can be any real number.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">Connections</code>. Each connection that departs from the input node has a weight associated with it and this can also be any real number.</p>
  </li>
  <li>
    <p>All the values of the input nodes and weights of the connections are brought together: they are used as inputs for a <code class="highlighter-rouge">weighted sum</code>.</p>
  </li>
  <li>
    <p>The result will be the input for a <code class="highlighter-rouge">transfer</code> or <code class="highlighter-rouge">activation function</code>.</p>
  </li>
  <li>
    <p>As a result, we have the <code class="highlighter-rouge">output node</code>, which is associated with the function (such as the sigmoid function) of the weighted sum of the input nodes.</p>
  </li>
  <li>
    <p>Finally, the <code class="highlighter-rouge">bias</code>, which we can actually consider as the weight associated with an additional input node that is permanently set to 1. The bias value is important because it allows us to shift the activation function to the left or right, which can determine the success of our learning.</p>
  </li>
</ol>

<p>The logical consequence of this model is that perceptrons only work with numerical data. This means that we need to convert any nominal data into a numerical format.</p>

<p>The perceptron can be used for classification purposes to determine that any output above a certain treshold indicates that an instance belongs to one class, while an output below the treshold might result in the input being a member of the other class. The straight line where the output equals the treshold is then the boundary between the two classes.</p>

<p><strong>Multi-Layer Perceptrons</strong></p>

<p>Networks of perceptrons are multi-layer perceptrons, and this is what we will implement in Python with the help of Keras. Multi-layer perceptrons are also known as “feed-forward neural networks”. These are more complex networks than the perceptron, as they consist of multiple neurons that are organized in layers. The number of layers is usually limited to two or three, but theoretically, there is no limit.</p>

<p>The layers act very much like the biological neurons, the outputs of one layer serve as the inputs for the next layer.</p>

<p>Among the layers, we can distinguish an input layer, hidden layers and an output layer. Multi-layer perceptrons are often fully connected. This means that there’s a connection from each perceptron in a certain layer to each perceptron in the next layer.</p>

<p>While the perceptron could only represent linear separations between classes, the multi-layer perceptron overcomes that limitation and can also represent more complex decision boundaries.</p>

<h1 id="2-the-data">2. The Data</h1>

<p><strong>Can we predict whether a wine is red or white by looking at its chemical properties?</strong></p>

<p>The data consists of two datasets that are related to red and white variants of the Portuguese “Vinho Verde” wine. Below is the Data Dictionary for the 12 variables that are included in the data:</p>

<ol>
  <li><code class="highlighter-rouge">Fixed acidity</code>: acids are major wine properties and contribute greatly to the wine’s taste. Usually, the total acidity is divided into two groups: the volatile acids and the nonvolatile or fixed acids. Among the fixed acids that we can find in wines are the following: tartaric, malic, citric, and succinic. This variable is expressed in g(tartaricacidtartaricacid)/dm3dm3 in the data sets.</li>
  <li><code class="highlighter-rouge">Volatile acidity</code>: the volatile acidity is basically the process of wine turning into vinegar. In the U.S, the legal limits of Volatile Acidity are 1.2 g/L for red table wine and 1.1 g/L for white table wine. In these data sets, the volatile acidity is expressed in g(aceticacidaceticacid)/dm3dm3.</li>
  <li><code class="highlighter-rouge">Citric acid</code> is one of the fixed acids that we will find in wines. It’s expressed in g/dm3dm3 in the two data sets.</li>
  <li><code class="highlighter-rouge">Residual sugar</code> typically refers to the sugar remaining after fermentation stops, or is stopped. It’s expressed in g/dm3dm3 in the red and white data.</li>
  <li><code class="highlighter-rouge">Chlorides</code> can be a major contributor to saltiness in wine. Here, we will see that it’s expressed in g(sodiumchloridesodiumchloride)/dm3dm3.</li>
  <li><code class="highlighter-rouge">Free sulfur dioxide</code>: the part of the sulphur dioxide that is added to a wine and that is lost into it is said to be bound, while the active part is said to be free. Winemaker will always try to get the highest proportion of free sulphur to bind. This variables is expressed in mg/dm3dm3 in the data.</li>
  <li><code class="highlighter-rouge">Total sulfur dioxide</code> is the sum of the bound and the free sulfur dioxide (SO2). Here, it’s expressed in mg/dm3dm3. There are legal limits for sulfur levels in wines: in the EU, red wines can only have 160mg/L, while white and rose wines can have about 210mg/L. Sweet wines are allowed to have 400mg/L. For the US, the legal limits are set at 350mg/L and for Australia, this is 250mg/L.</li>
  <li><code class="highlighter-rouge">Density</code> is generally used as a measure of the conversion of sugar to alcohol. Here, it’s expressed in g/cm3cm3.</li>
  <li><code class="highlighter-rouge">pH</code> or the potential of hydrogen is a numeric scale to specify the acidity or basicity the wine. As we might know, solutions with a pH less than 7 are acidic, while solutions with a pH greater than 7 are basic. With a pH of 7, pure water is neutral. Most wines have a pH between 2.9 and 3.9 and are therefore acidic.</li>
  <li><code class="highlighter-rouge">Sulphates</code> are to wine as gluten is to food. We might already know sulphites from the wine have been known to cause headaches. They are a regular part of winemaking and are considered necessary to produce wine. In this case, they are expressed in g(potassiumsulphatepotassiumsulphate)/dm3dm3.</li>
  <li><code class="highlighter-rouge">Alcohol</code>: wine is an alcoholic beverage and as we know, the percentage of alcohol can vary from wine to wine. It’s expressed as % vol in the red and white data.</li>
  <li><code class="highlighter-rouge">Quality</code>: wine experts graded the wine quality between 0 (very bad) and 10 (very excellent). The eventual number is the median of at least three evaluations made by those same wine experts.</li>
</ol>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Import numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c"># Import pandas </span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="c"># Read in white wine data </span>
<span class="n">white</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>

<span class="c"># Read in red wine data </span>
<span class="n">red</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>
</code></pre>
</div>

<h1 id="3-exploratory-data-analysis">3. Exploratory Data Analysis</h1>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Print info on white wine</span>
<span class="k">print</span><span class="p">(</span><span class="s">"White Wine:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">white</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>

<span class="c"># Print info on red wine</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""Red Wine:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">red</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>White Wine:
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 4898 entries, 0 to 4897
Data columns (total 12 columns):
fixed acidity           4898 non-null float64
volatile acidity        4898 non-null float64
citric acid             4898 non-null float64
residual sugar          4898 non-null float64
chlorides               4898 non-null float64
free sulfur dioxide     4898 non-null float64
total sulfur dioxide    4898 non-null float64
density                 4898 non-null float64
pH                      4898 non-null float64
sulphates               4898 non-null float64
alcohol                 4898 non-null float64
quality                 4898 non-null int64
dtypes: float64(11), int64(1)
memory usage: 459.3 KB
None

Red Wine:
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1599 entries, 0 to 1598
Data columns (total 12 columns):
fixed acidity           1599 non-null float64
volatile acidity        1599 non-null float64
citric acid             1599 non-null float64
residual sugar          1599 non-null float64
chlorides               1599 non-null float64
free sulfur dioxide     1599 non-null float64
total sulfur dioxide    1599 non-null float64
density                 1599 non-null float64
pH                      1599 non-null float64
sulphates               1599 non-null float64
alcohol                 1599 non-null float64
quality                 1599 non-null int64
dtypes: float64(11), int64(1)
memory usage: 150.0 KB
None
</code></pre>
</div>

<p>View the first 5 rows of <code class="highlighter-rouge">red</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">red</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.8</td>
      <td>0.88</td>
      <td>0.00</td>
      <td>2.6</td>
      <td>0.098</td>
      <td>25.0</td>
      <td>67.0</td>
      <td>0.9968</td>
      <td>3.20</td>
      <td>0.68</td>
      <td>9.8</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.8</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>2.3</td>
      <td>0.092</td>
      <td>15.0</td>
      <td>54.0</td>
      <td>0.9970</td>
      <td>3.26</td>
      <td>0.65</td>
      <td>9.8</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.2</td>
      <td>0.28</td>
      <td>0.56</td>
      <td>1.9</td>
      <td>0.075</td>
      <td>17.0</td>
      <td>60.0</td>
      <td>0.9980</td>
      <td>3.16</td>
      <td>0.58</td>
      <td>9.8</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>View the last 5 rows of <code class="highlighter-rouge">white</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">white</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4893</th>
      <td>6.2</td>
      <td>0.21</td>
      <td>0.29</td>
      <td>1.6</td>
      <td>0.039</td>
      <td>24.0</td>
      <td>92.0</td>
      <td>0.99114</td>
      <td>3.27</td>
      <td>0.50</td>
      <td>11.2</td>
      <td>6</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4894</th>
      <td>6.6</td>
      <td>0.32</td>
      <td>0.36</td>
      <td>8.0</td>
      <td>0.047</td>
      <td>57.0</td>
      <td>168.0</td>
      <td>0.99490</td>
      <td>3.15</td>
      <td>0.46</td>
      <td>9.6</td>
      <td>5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4895</th>
      <td>6.5</td>
      <td>0.24</td>
      <td>0.19</td>
      <td>1.2</td>
      <td>0.041</td>
      <td>30.0</td>
      <td>111.0</td>
      <td>0.99254</td>
      <td>2.99</td>
      <td>0.46</td>
      <td>9.4</td>
      <td>6</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4896</th>
      <td>5.5</td>
      <td>0.29</td>
      <td>0.30</td>
      <td>1.1</td>
      <td>0.022</td>
      <td>20.0</td>
      <td>110.0</td>
      <td>0.98869</td>
      <td>3.34</td>
      <td>0.38</td>
      <td>12.8</td>
      <td>7</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4897</th>
      <td>6.0</td>
      <td>0.21</td>
      <td>0.38</td>
      <td>0.8</td>
      <td>0.020</td>
      <td>22.0</td>
      <td>98.0</td>
      <td>0.98941</td>
      <td>3.26</td>
      <td>0.32</td>
      <td>11.8</td>
      <td>6</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Sample 5 rows of <code class="highlighter-rouge">red</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">red</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1116</th>
      <td>7.0</td>
      <td>0.69</td>
      <td>0.07</td>
      <td>2.5</td>
      <td>0.091</td>
      <td>15.0</td>
      <td>21.0</td>
      <td>0.99572</td>
      <td>3.38</td>
      <td>0.60</td>
      <td>11.3</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1099</th>
      <td>8.6</td>
      <td>0.52</td>
      <td>0.38</td>
      <td>1.5</td>
      <td>0.096</td>
      <td>5.0</td>
      <td>18.0</td>
      <td>0.99666</td>
      <td>3.20</td>
      <td>0.52</td>
      <td>9.4</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>680</th>
      <td>13.3</td>
      <td>0.43</td>
      <td>0.58</td>
      <td>1.9</td>
      <td>0.070</td>
      <td>15.0</td>
      <td>40.0</td>
      <td>1.00040</td>
      <td>3.06</td>
      <td>0.49</td>
      <td>9.0</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1123</th>
      <td>10.7</td>
      <td>0.40</td>
      <td>0.37</td>
      <td>1.9</td>
      <td>0.081</td>
      <td>17.0</td>
      <td>29.0</td>
      <td>0.99674</td>
      <td>3.12</td>
      <td>0.65</td>
      <td>11.2</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1509</th>
      <td>7.9</td>
      <td>0.18</td>
      <td>0.40</td>
      <td>1.8</td>
      <td>0.062</td>
      <td>7.0</td>
      <td>20.0</td>
      <td>0.99410</td>
      <td>3.28</td>
      <td>0.70</td>
      <td>11.1</td>
      <td>5</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p><code class="highlighter-rouge">Summary Statistics</code> about our data to assess its quality:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Describe `white`</span>
<span class="n">white</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.000000</td>
      <td>4898.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>6.854788</td>
      <td>0.278241</td>
      <td>0.334192</td>
      <td>6.391415</td>
      <td>0.045772</td>
      <td>35.308085</td>
      <td>138.360657</td>
      <td>0.994027</td>
      <td>3.188267</td>
      <td>0.489847</td>
      <td>10.514267</td>
      <td>5.877909</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.843868</td>
      <td>0.100795</td>
      <td>0.121020</td>
      <td>5.072058</td>
      <td>0.021848</td>
      <td>17.007137</td>
      <td>42.498065</td>
      <td>0.002991</td>
      <td>0.151001</td>
      <td>0.114126</td>
      <td>1.230621</td>
      <td>0.885639</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>3.800000</td>
      <td>0.080000</td>
      <td>0.000000</td>
      <td>0.600000</td>
      <td>0.009000</td>
      <td>2.000000</td>
      <td>9.000000</td>
      <td>0.987110</td>
      <td>2.720000</td>
      <td>0.220000</td>
      <td>8.000000</td>
      <td>3.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>6.300000</td>
      <td>0.210000</td>
      <td>0.270000</td>
      <td>1.700000</td>
      <td>0.036000</td>
      <td>23.000000</td>
      <td>108.000000</td>
      <td>0.991723</td>
      <td>3.090000</td>
      <td>0.410000</td>
      <td>9.500000</td>
      <td>5.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>6.800000</td>
      <td>0.260000</td>
      <td>0.320000</td>
      <td>5.200000</td>
      <td>0.043000</td>
      <td>34.000000</td>
      <td>134.000000</td>
      <td>0.993740</td>
      <td>3.180000</td>
      <td>0.470000</td>
      <td>10.400000</td>
      <td>6.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>7.300000</td>
      <td>0.320000</td>
      <td>0.390000</td>
      <td>9.900000</td>
      <td>0.050000</td>
      <td>46.000000</td>
      <td>167.000000</td>
      <td>0.996100</td>
      <td>3.280000</td>
      <td>0.550000</td>
      <td>11.400000</td>
      <td>6.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>14.200000</td>
      <td>1.100000</td>
      <td>1.660000</td>
      <td>65.800000</td>
      <td>0.346000</td>
      <td>289.000000</td>
      <td>440.000000</td>
      <td>1.038980</td>
      <td>3.820000</td>
      <td>1.080000</td>
      <td>14.200000</td>
      <td>9.000000</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>It is very important to be aware of differences amongst the variables. We can see that some of the variables have a lot of difference in their <code class="highlighter-rouge">min</code> and <code class="highlighter-rouge">max</code> values.</p>

<p>Double check for <code class="highlighter-rouge">null</code> values:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"White Wine:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">isnull</span><span class="p">(</span><span class="n">white</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""Red Wine:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">isnull</span><span class="p">(</span><span class="n">red</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>White Wine:
fixed acidity           4898
volatile acidity        4898
citric acid             4898
residual sugar          4898
chlorides               4898
free sulfur dioxide     4898
total sulfur dioxide    4898
density                 4898
pH                      4898
sulphates               4898
alcohol                 4898
quality                 4898
dtype: int64

Red Wine:
fixed acidity           1599
volatile acidity        1599
citric acid             1599
residual sugar          1599
chlorides               1599
free sulfur dioxide     1599
total sulfur dioxide    1599
density                 1599
pH                      1599
sulphates               1599
alcohol                 1599
quality                 1599
dtype: int64
</code></pre>
</div>

<h2 id="31-visualizing-the-data">3.1 Visualizing The Data</h2>
<p>Let’s take a look at the distribution of some of the dataset’s variables and make scatter plots to see possible correlations.</p>

<p><strong>Alcohol</strong></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">red</span><span class="o">.</span><span class="n">alcohol</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Red wine"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">white</span><span class="o">.</span><span class="n">alcohol</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'white'</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"White wine"</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Alcohol in </span><span class="si">% </span><span class="s">Vol"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Alcohol in </span><span class="si">% </span><span class="s">Vol"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Frequency"</span><span class="p">)</span>
<span class="c">#ax[0].legend(loc='best')</span>
<span class="c">#ax[1].legend(loc='best')</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">"Distribution of Alcohol in </span><span class="si">% </span><span class="s">Vol"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/assets/media/wine/WineType_21_0.png" alt="png" /></p>

<p>We can see that the alcohol levels between the red and white wine are mostly the same: they have around 9% of alcohol. There are also a considerable amount of observations that have 10% or 11% of alcohol percentage.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># print(np.histogram(red.alcohol, bins=[7,8,9,10,11,12,13,14,15]))</span>
<span class="c"># print(np.histogram(white.alcohol, bins=[7,8,9,10,11,12,13,14,15]))</span>
</code></pre>
</div>

<p><strong>Sulphates</strong><br />
One thing that we should look at is the relationship between the <code class="highlighter-rouge">sulphates</code> and the <code class="highlighter-rouge">quality</code> of the wine. High levels of sulphate are known to cause headaches and may influence the quality of the wine as well as its rating.</p>

<p>Let’s take a look:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">red</span><span class="p">[</span><span class="s">'quality'</span><span class="p">],</span> <span class="n">red</span><span class="p">[</span><span class="s">"sulphates"</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">white</span><span class="p">[</span><span class="s">'quality'</span><span class="p">],</span> <span class="n">white</span><span class="p">[</span><span class="s">'sulphates'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">"white"</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s">"black"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Red Wine"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"White Wine"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Quality"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Quality"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Sulphates"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Sulphates"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.5</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">"Wine Quality by Amount of Sulphates"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/assets/media/wine/WineType_25_0.png" alt="png" /></p>

<p>Red wine seems to contain more sulphates than white wine, which only has a couple of exceptions that fall just above 1 g/dm3dm3. This could maybe explain why red wine causes headaches.</p>

<p>In regards to the quality, we can clearly see that there is white wine with a relatively low amount of sulphates that gets a score of 9, but for the rest it’s difficult to interpret the data correctly at this point.</p>

<p>We need to take into account that the difference in observations could also affect the graphs and how we might interpret them.</p>

<p><strong>Acidity</strong><br />
Apart from the sulphates, the acidity is one of the major and important wine characteristics that is necessary to achieve quality wines. Great wines often balance out acidity, tannin, alcohol and sweetness. In quantities of 0.2 to 0.4 g/L, volatile acidity doesn’t affect a wine’s quality. At higher levels, however, volatile acidity can give wine a sharp, vinegary tactile sensation. Extreme volatile acidity signifies a seriously flawed wine.</p>

<p>Let’s put the data to the test and make a scatter plot that plots the alcohol versus the volatile acidity. The data points will be colored according to their rating or <code class="highlighter-rouge">quality</code> label:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">570</span><span class="p">)</span>

<span class="n">redlabels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">red</span><span class="p">[</span><span class="s">'quality'</span><span class="p">])</span>
<span class="n">whitelabels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">white</span><span class="p">[</span><span class="s">'quality'</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">redcolors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">whitecolors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">redcolors</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">redcolors</span><span class="p">)):</span>
    <span class="n">redy</span> <span class="o">=</span> <span class="n">red</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">][</span><span class="n">red</span><span class="o">.</span><span class="n">quality</span> <span class="o">==</span> <span class="n">redlabels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">redx</span> <span class="o">=</span> <span class="n">red</span><span class="p">[</span><span class="s">'volatile acidity'</span><span class="p">][</span><span class="n">red</span><span class="o">.</span><span class="n">quality</span> <span class="o">==</span> <span class="n">redlabels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">redx</span><span class="p">,</span> <span class="n">redy</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">redcolors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">whitecolors</span><span class="p">)):</span>
    <span class="n">whitey</span> <span class="o">=</span> <span class="n">white</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">][</span><span class="n">white</span><span class="o">.</span><span class="n">quality</span> <span class="o">==</span> <span class="n">whitelabels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">whitex</span> <span class="o">=</span> <span class="n">white</span><span class="p">[</span><span class="s">'volatile acidity'</span><span class="p">][</span><span class="n">white</span><span class="o">.</span><span class="n">quality</span> <span class="o">==</span> <span class="n">whitelabels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">whitex</span><span class="p">,</span> <span class="n">whitey</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">whitecolors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Red Wine"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"White Wine"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.7</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.7</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mf">15.5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mf">15.5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Volatile Acidity"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Alcohol"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Volatile Acidity"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Alcohol"</span><span class="p">)</span> 
<span class="c">#ax[0].legend(redlabels, loc='best', bbox_to_anchor=(1.3, 1))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">whitelabels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c">#fig.suptitle("Alcohol - Volatile Acidity")</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/assets/media/wine/WineType_28_0.png" alt="png" /></p>

<p>The colors in this image are randomly chosen with the help of the numpy <code class="highlighter-rouge">random</code> module. We can always change this by passing a list to the <code class="highlighter-rouge">redcolors</code> or <code class="highlighter-rouge">whitecolors</code> variables.</p>

<p>Keep in mind that the white wine data has one unique <code class="highlighter-rouge">quality</code> value more than the red wine data.</p>

<h2 id="32-data-observation">3.2 Data Observation</h2>

<ul>
  <li>Some of the variables of our data sets have values that are considerably far apart</li>
  <li>We have an ideal scenario: there are no null values in the data sets</li>
  <li>Most wines that were included in the data set have around 9% of alcohol</li>
  <li>Red wine seems to contain more sulphates than the white wine, which has less sulphates above 1 g/dm3dm3</li>
  <li>Most wines had a volatile acidity of 0.5 and below. At the moment, there is no direct relation to the quality of the wine</li>
</ul>

<h1 id="4-preprocess-data">4. Preprocess Data</h1>
<p>Let’s preprocess the data so we can start building our neural network.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Add `type` column to `red` with value 1</span>
<span class="n">red</span><span class="p">[</span><span class="s">'type'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c"># Add `type` column to `white` with value 0</span>
<span class="n">white</span><span class="p">[</span><span class="s">'type'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c"># Append `white` to `red`</span>
<span class="n">wines</span> <span class="o">=</span> <span class="n">red</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">white</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre>
</div>

<p>We set <code class="highlighter-rouge">ignore_index</code> to <code class="highlighter-rouge">True</code> in this case because we don’t want to keep the index labels of <code class="highlighter-rouge">white</code> when we are appending the data to <code class="highlighter-rouge">red</code>: we want the labels to continue from where they left off in <code class="highlighter-rouge">red</code>, not duplicate index labels from joining both data sets together.</p>

<p><strong>Correlation Matrix</strong><br />
Since it can be somewhat difficult to interpret graphs, it’s also a good idea to plot a correlation matrix. This will give insights more quickly about which variables correlate:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="n">corr</span> <span class="o">=</span> <span class="n">wines</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr</span><span class="p">,</span> 
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">corr</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">yticklabels</span><span class="o">=</span><span class="n">corr</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/assets/media/wine/WineType_35_0.png" alt="png" /></p>

<p>As we would expect, there are some variables that correlate, such as <code class="highlighter-rouge">density</code> and <code class="highlighter-rouge">residual sugar</code>. Also <code class="highlighter-rouge">volatile acidity</code> and <code class="highlighter-rouge">type</code> are more closely connected than we originally thought.</p>

<h1 id="5-train-and-test-sets">5. Train and Test Sets</h1>
<p>Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. Most classification data sets do not have exactly equal number of instances in each class, but a small difference often does not matter.</p>

<p>We need to make sure that all two classes of wine are present in the training model. The amount of instances of all two wine types needs to be more or less equal so that we do not favor one class over the other in our predictions.</p>

<p>We can import the <code class="highlighter-rouge">train_test_split</code> from <code class="highlighter-rouge">sklearn.model_selection</code> and assign the data and the target labels to the variables <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">y</code>. We will see that we need to flatten the array of target labels in order to be completely ready to use the <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">y</code> variables as input for the <code class="highlighter-rouge">train_test_split()</code> function.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Import `train_test_split` from `sklearn.model_selection`</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c"># Specify the data </span>
<span class="n">X</span><span class="o">=</span><span class="n">wines</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">11</span><span class="p">]</span>

<span class="c"># Specify the target labels and flatten the array</span>
<span class="n">y</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">wines</span><span class="o">.</span><span class="nb">type</span><span class="p">)</span>

<span class="c"># Split the data up in train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre>
</div>

<h1 id="6-standardize-the-data">6. Standardize The Data</h1>
<p>Some of the values in the <code class="highlighter-rouge">white</code> and <code class="highlighter-rouge">red</code> data sets are far apart and we need to do some standardization to work with these values.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Import `StandardScaler` from `sklearn.preprocessing`</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c"># Define the scaler </span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c"># Scale the train set</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c"># Scale the test set</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre>
</div>

<h1 id="7-model-data">7. Model Data</h1>
<p>Since we only have two classes, white and red, we are going to do a binary classification. We have already encoded red as 1 and white as 0.</p>

<p>A type of network that performs well on such a problem is a multi-layer perceptron. We are looking to build a fairly simple stack of fully-connected layers to solve our problem and we will use the <code class="highlighter-rouge">relu</code> activation fuction.</p>

<p>A quick way to get started on building our multi-layer perceptron is to use the Keras Sequential model: it’s a linear stack of layers. We can easily create the model by passing a list of layer instances to the constructor, which is set up by running <code class="highlighter-rouge">model = Sequential()</code>.</p>

<p>When making our model, it’s important to take into account that our first layer needs to make the input shape clear. The model needs to know what input shape to expect and that’s why we will always find the <code class="highlighter-rouge">input_shape</code>, <code class="highlighter-rouge">input_dim</code>, <code class="highlighter-rouge">input_length</code>, or <code class="highlighter-rouge">batch_size</code> arguments from the <a href="https://keras.io/getting-started/sequential-model-guide/">documentation</a>.</p>

<p>In this case, we have to use a <code class="highlighter-rouge">Dense</code> layer, which is a fully connected layer. Dense layers implement the following operation: <code class="highlighter-rouge">output = activation(dot(input, kernel) + bias)</code>. Note that without the activation function, our Dense layer would only consist of two linear operations: a dot product and an addition.</p>

<p>In the first layer, the <code class="highlighter-rouge">activation</code> argument takes the value <code class="highlighter-rouge">relu</code>. Next, we also see that the <code class="highlighter-rouge">input_shape</code> has been defined. This is the <code class="highlighter-rouge">input</code> of the operation that we have just seen: the model takes as input arrays of shape <code class="highlighter-rouge">(12,)</code>, or <code class="highlighter-rouge">(*, 12)</code>. Finally, we can see that the first layer has <code class="highlighter-rouge">12</code> as a first value for the <code class="highlighter-rouge">units</code> argument of <code class="highlighter-rouge">Dense()</code>, which is the dimensionality of the output space, which are actually 12 hidden units. This means that the model will output arrays of shape <code class="highlighter-rouge">(*, 12)</code>: this is is the dimensionality of the output space.</p>

<p>The <code class="highlighter-rouge">units</code> actually represents the <code class="highlighter-rouge">kernel</code> of the above formula or the weights matrix, composed of all weights given to all input nodes, created by the layer. Note that we don’t include any bias because we haven’t included the <code class="highlighter-rouge">use_bias</code> argument and set it to <code class="highlighter-rouge">TRUE</code>, which is also a possibility.</p>

<p>The intermediate layer also uses the <code class="highlighter-rouge">relu</code> activation function. The output of this layer will be arrays of shape <code class="highlighter-rouge">(*,8)</code>.</p>

<p>We are ending the network with a <code class="highlighter-rouge">Dense</code> layer of size 1. The final layer will also use a <code class="highlighter-rouge">sigmoid</code> activation function so that our output is actually a probability; This means that this will result in a score between 0 and 1, indicating how likely the sample is to have the target “1”, or how likely the wine is to be red.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Import `Sequential` from `keras.models`</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="c"># Import `Dense` from `keras.layers`</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="c"># Initialize the constructor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c"># Add an input layer </span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,)))</span>

<span class="c"># Add one hidden layer </span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># Add an output layer </span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Using TensorFlow backend.
</code></pre>
</div>

<p>We see that there are two key architecture decisions that we need to make to build our model:</p>
<ul>
  <li>How many layers we are going to use and</li>
  <li>How many “hidden units” we will chose for each layer</li>
</ul>

<p>In this case, we picked <code class="highlighter-rouge">12</code> hidden units for the first layer of our model: this is the dimensionality of the output space. We are setting the amount of freedom that we are allowing the network to have when it’s learning representations. If we would allow more hidden units, our network will be able to learn more complex representations but it will also be a more expensive operations that can be prone to overfitting. Overfitting occurs when the model is too complex: it will describe random error or noise and not the underlying relationship that it needs to describe.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Model output shape</span>
<span class="n">model</span><span class="o">.</span><span class="n">output_shape</span>

<span class="c"># Model summary</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c"># Model config</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>

<span class="c"># List all weight tensors </span>
<span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 12)                144       
_________________________________________________________________
dense_2 (Dense)              (None, 8)                 104       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 9         
=================================================================
Total params: 257
Trainable params: 257
Non-trainable params: 0
_________________________________________________________________





[array([[ 0.2896654 ,  0.01695609, -0.1637314 , -0.4055706 , -0.1897687 ,
          0.05104744, -0.47552979,  0.04375035,  0.14994401,  0.18585712,
          0.04605234,  0.4203161 ],
        [ 0.09246093, -0.00486219,  0.15394145, -0.3556453 ,  0.37075537,
         -0.09201503,  0.22433394,  0.49172276,  0.02939814, -0.00528288,
         -0.28554958, -0.29101855],
        [ 0.13772285,  0.10540932, -0.16185051, -0.13713309, -0.30931568,
          0.29137081,  0.22610158, -0.03789827,  0.03900152, -0.13670495,
          0.02206749,  0.07224816],
        [ 0.12972063, -0.46136355, -0.1358273 , -0.03706533,  0.21573442,
          0.21198452, -0.02762926, -0.19320443,  0.24703926, -0.42241281,
          0.29690397, -0.07473344],
        [-0.06937274,  0.13956004, -0.04767117,  0.03820831,  0.27192348,
         -0.47782913, -0.09866273, -0.42932004, -0.04532751, -0.17295876,
         -0.34326571,  0.48160571],
        [ 0.06270897,  0.10312521,  0.34606183, -0.12457022, -0.16116223,
          0.03542614,  0.15460205, -0.48483256, -0.21644399,  0.17074299,
         -0.31874129, -0.00338858],
        [-0.12783119, -0.04838416,  0.20839161,  0.33239806,  0.10142457,
         -0.30532336, -0.38364607,  0.38588023,  0.01854885,  0.05628592,
          0.19293177, -0.05149972],
        [-0.43119535, -0.27155614, -0.46491459, -0.25988257,  0.09503597,
         -0.5009988 , -0.15289614, -0.41986507,  0.49207467,  0.00157392,
         -0.16561732,  0.24532884],
        [-0.17463535,  0.08331698,  0.27836359, -0.14132953, -0.30007723,
         -0.32280877, -0.1596224 ,  0.32339281, -0.29511583,  0.41530758,
         -0.45230752, -0.4677496 ],
        [ 0.00880128,  0.33655512, -0.45286989,  0.43754292, -0.04207084,
         -0.3803001 ,  0.21270508,  0.43465519,  0.46255559, -0.18048412,
         -0.29980046,  0.45833129],
        [-0.31532359, -0.26553845, -0.21347395, -0.48159593,  0.50684947,
          0.29534268,  0.16204995,  0.30744392,  0.02605116, -0.42019787,
         -0.2871666 ,  0.01862645]], dtype=float32),
 array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),
 array([[ 0.13802433, -0.1397422 ,  0.37863284, -0.15413004,  0.52855718,
         -0.13628149,  0.03936303, -0.25822985],
        [-0.49231729, -0.38898361, -0.35550442, -0.08421832, -0.25332853,
         -0.01050067,  0.28014278,  0.45117211],
        [ 0.12644804, -0.07388979,  0.54314303,  0.32438159,  0.36960304,
          0.03181666, -0.4164722 , -0.5394448 ],
        [ 0.4500739 ,  0.19391423, -0.21727106, -0.30200374, -0.48891065,
          0.42746753, -0.34170333, -0.21399096],
        [-0.53348607, -0.34708104, -0.44702923,  0.31050652, -0.52960402,
          0.4368825 , -0.46614015,  0.21192652],
        [-0.50510532,  0.35415596, -0.37515181, -0.2976757 , -0.45574439,
         -0.53875005,  0.50481069,  0.12087846],
        [ 0.49851513, -0.30494523,  0.10521847, -0.46860957,  0.2679804 ,
         -0.04077768, -0.00542253,  0.31112713],
        [-0.34450638, -0.16991124, -0.38176683, -0.38524371, -0.49763155,
          0.37959671,  0.42052734,  0.38327682],
        [-0.36905417,  0.40031129, -0.1917389 ,  0.21109205,  0.33099413,
          0.1206221 , -0.02995032, -0.21631998],
        [ 0.0382123 ,  0.51903856,  0.31960887, -0.01158792, -0.05701348,
          0.49415326, -0.36317486, -0.18171829],
        [ 0.42716026,  0.4860462 ,  0.29231024, -0.49826044,  0.38350588,
          0.51668561, -0.35148156,  0.00427073],
        [ 0.14594537, -0.22531378,  0.36939985, -0.14032108, -0.48043332,
         -0.00821644,  0.54728377, -0.17121464]], dtype=float32),
 array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),
 array([[ 0.76886392],
        [ 0.16227716],
        [ 0.38850141],
        [-0.30456817],
        [ 0.39023936],
        [-0.56345725],
        [-0.12128478],
        [ 0.20257616]], dtype=float32),
 array([ 0.], dtype=float32)]
</code></pre>
</div>

<h1 id="8-compile-and-fit">8. Compile and Fit</h1>
<p>Now we need to compile our model and fit it to the data.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
                   
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Epoch 1/20
4352/4352 [==============================] - 14s - loss: 0.0901 - acc: 0.9692    
Epoch 2/20
4352/4352 [==============================] - 16s - loss: 0.0221 - acc: 0.9959    
Epoch 3/20
4352/4352 [==============================] - 15s - loss: 0.0195 - acc: 0.9966    
Epoch 4/20
4352/4352 [==============================] - 14s - loss: 0.0166 - acc: 0.9972    
Epoch 5/20
4352/4352 [==============================] - 15s - loss: 0.0152 - acc: 0.9975    
Epoch 6/20
4352/4352 [==============================] - 15s - loss: 0.0141 - acc: 0.9972    
Epoch 7/20
4352/4352 [==============================] - 15s - loss: 0.0133 - acc: 0.9972    
Epoch 8/20
4352/4352 [==============================] - 16s - loss: 0.0152 - acc: 0.9972    
Epoch 9/20
4352/4352 [==============================] - 14s - loss: 0.0116 - acc: 0.9977    
Epoch 10/20
4352/4352 [==============================] - 15s - loss: 0.0110 - acc: 0.9977    
Epoch 11/20
4352/4352 [==============================] - 15s - loss: 0.0113 - acc: 0.9977    
Epoch 12/20
4352/4352 [==============================] - 15s - loss: 0.0110 - acc: 0.9975    
Epoch 13/20
4352/4352 [==============================] - 15s - loss: 0.0102 - acc: 0.9979    
Epoch 14/20
4352/4352 [==============================] - 15s - loss: 0.0098 - acc: 0.9982    
Epoch 15/20
4352/4352 [==============================] - 14s - loss: 0.0106 - acc: 0.9979    
Epoch 16/20
4352/4352 [==============================] - 14s - loss: 0.0108 - acc: 0.9977    
Epoch 17/20
4352/4352 [==============================] - 16s - loss: 0.0094 - acc: 0.9979    
Epoch 18/20
4352/4352 [==============================] - 15s - loss: 0.0096 - acc: 0.9979    
Epoch 19/20
4352/4352 [==============================] - 15s - loss: 0.0100 - acc: 0.9979    
Epoch 20/20
4352/4352 [==============================] - 15s - loss: 0.0082 - acc: 0.9984    





&lt;keras.callbacks.History at 0x127d38cc0&gt;
</code></pre>
</div>

<p>By compiling, we are configure the model with the <code class="highlighter-rouge">adam</code> optimizer and the <code class="highlighter-rouge">binary_crossentropy</code> loss function. Additionally, we can also monitor the accuracy during the training by passing <code class="highlighter-rouge">['accuracy']</code> to the <code class="highlighter-rouge">metrics</code> argument.</p>

<p>The <code class="highlighter-rouge">optimizer</code> and the <code class="highlighter-rouge">loss</code> are two arguments that are required if we want to compile the model. Some of the most popular optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop. Depending on whichever algorithm we choose, we will need to tune certain parameters, such as learning rate or momentum. The choice for a loss function depends on the problem we are trying to solve.</p>

<p>We will train the model for 20 epochs or iterations over all the samples in <code class="highlighter-rouge">X_train</code> and <code class="highlighter-rouge">y_train</code>, in batches of 1 sample. By setting the <code class="highlighter-rouge">verbose</code> argument to <code class="highlighter-rouge">1</code>, we are indicate that we want to see the progress bar logging.</p>

<p>An epoch is a single pass through the entire training set, followed by testing of the verification set. The batch size that we specify in the code above defines the number of samples that going to be propagated through the network. Also, by doing this, we optimize the efficiency because we are making sure that we don’t load too many input patterns into memory at the same time.</p>

<h1 id="9-predict-values">9. Predict Values</h1>
<p>We can make predictions for the labels of the test set by using <code class="highlighter-rouge">predict()</code> and passing the test set to it. In this case, the result is stored in <code class="highlighter-rouge">y_pred</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre>
</div>

<p>Before we go and evaluate our model, we can already get a quick idea of the accuracy by checking how <code class="highlighter-rouge">y_pred</code> and <code class="highlighter-rouge">y_test</code> compare:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"y_pred:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""y_test:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>y_pred:
[[  1.23926289e-02]
 [  8.67547810e-01]
 [  2.65277573e-04]
 [  1.73703957e-05]
 [  1.84766762e-07]]

y_test:
[0 1 0 0 0]
</code></pre>
</div>

<h1 id="10-evaluate-model">10. Evaluate Model</h1>
<p>We will use <code class="highlighter-rouge">evaluate()</code> and pass in the test data and test labels to evaluate our model’s performance.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>1504/2145 [====================&gt;.........] - ETA: 0s[0.027259037624980813, 0.99487179487179489]
</code></pre>
</div>

<p>The score is a list that holds the combination of the loss and the accuracy. In this case, we see that both seem very great, but we also need to remember that our data is somewhat imbalanced, we had more white wine than red wine observations. The accuracy might just be reflecting the class distribution of our data because it will just predict <code class="highlighter-rouge">white</code> because those observations are abundantly present.</p>

<p>Before we start re-arranging the data and putting it together in different ways, it’s always a good idea to try out different evaluation metrics. For this, we will test out some basic classification evaluation techniques, such as:</p>

<ul>
  <li>The <code class="highlighter-rouge">confusion matrix</code>, which is a breakdown of predictions into a table showing correct predictions and the types of incorrect predictions made. Ideally, we will only see numbers in the diagonal, which means that all our predictions were correct.</li>
  <li><code class="highlighter-rouge">Precision</code> is a measure of a classifier’s exactness. The higher the precision, the more accurate the classifier.</li>
  <li><code class="highlighter-rouge">Recall</code> is a measure of a classifier’s completeness. The higher the recall, the more cases the classifier covers.</li>
  <li>The <code class="highlighter-rouge">F1 Score</code> or <code class="highlighter-rouge">F-score</code> is a weighted average of precision and recall.</li>
  <li>The <code class="highlighter-rouge">Kappa</code> or <code class="highlighter-rouge">Cohen’s kappa</code> is the classification accuracy normalized by the imbalance of the classes in the data.</li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Import the modules from `sklearn.metrics`</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">cohen_kappa_score</span>

<span class="c"># Confusion matrix</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Confusion Matrix:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre>
</div>

<p>array([[1585,    3],
       [   8,  549]])</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Precision </span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""Precision:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre>
</div>

<p>0.994565217391</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Recall</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""Recall:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre>
</div>

<p>0.98563734290843807</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># F1 score</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""F1 Score:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
</code></pre>
</div>

<p>0.99008115419296661</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Cohen's kappa</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""Cohen's Kappa:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">cohen_kappa_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre>
</div>

<p>0.98662321692498967</p>

<p>All of our scores are very good, we have made a pretty accurate model despite the fact that we have considerably more rows that are of the white wine type.</p>

<h1 id="11-further-modeling">11. Further Modeling</h1>
<p>We have successfully built our first model, but we can go even further with experimenting. The two key architectural decisions that we need to make involve the layers and the hidden nodes. We can start by testing the following:</p>

<ul>
  <li>Try to use 2 or 3 hidden layers</li>
  <li>Use layers with more hidden units or less hidden units</li>
  <li>Take the quality column as the target labels and the rest of the data including the encoded type column as our data. Which leaves us with a multi-class classification problem to solve.</li>
  <li>Try changing out the activation function by using the <code class="highlighter-rouge">tanh</code> activation function instead of <code class="highlighter-rouge">relu</code>.</li>
</ul>

<h1 id="12-predicting-wine-quality">12. Predicting Wine Quality</h1>

<p>** Lets build a neural networks to predict wine quality based on its chemical properties.**</p>

<p>Here we assume that <code class="highlighter-rouge">quality</code> is a continuous variable: the task is then not a <code class="highlighter-rouge">binary classification</code> task but an <code class="highlighter-rouge">ordinal regression</code> task. Ordinal Regression is a type of regression that is used for predicting an ordinal variable: the <code class="highlighter-rouge">quality</code> value exists on an arbitrary scale where the relative ordering between the different <code class="highlighter-rouge">quality</code> values is significant. The quality scale 0-10 for “very bad” to “very good” is an example.</p>

<p>Our target labels are going to be the <code class="highlighter-rouge">quality</code> column in our <code class="highlighter-rouge">red</code> and <code class="highlighter-rouge">white</code> DataFrames and will require some additional preprocessing.</p>

<h2 id="121-preprocess-data">12.1 Preprocess Data</h2>

<p>Since the <code class="highlighter-rouge">quality</code> variable becomes our target class, we will now need to isolate the quality labels from the rest of the data set. We will put <code class="highlighter-rouge">wines.quality</code> in a different variable <code class="highlighter-rouge">y</code> and will put the rest of the wine data in a variable <code class="highlighter-rouge">x</code>.</p>

<p>We will use <code class="highlighter-rouge">StandardScaler</code> to make sure that our data is in a good place before we fit the data to the model.</p>

<p>Then we will split the data into train and test sets by utilizing <code class="highlighter-rouge">k-fold</code> validation, which requires us to split up the data into <code class="highlighter-rouge">K</code> partitions. Usually, <code class="highlighter-rouge">K</code> is set at 4 or 5. Finally, we can instantiate identical models and train each one on a partition, while also evaluating on the remaining partitions. The validation score for the model is then an average of the <code class="highlighter-rouge">K</code> validation scores obtained.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Isolate target labels</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">wines</span><span class="o">.</span><span class="n">quality</span>

<span class="c"># Isolate data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">wines</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'quality'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
</code></pre>
</div>

<p>We also need to perform the scaling again because we had a lot of differences in some of the values for our <code class="highlighter-rouge">red</code>, <code class="highlighter-rouge">white</code> data.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Scale the data with `StandardScaler`</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre>
</div>

<p>Time to start modeling our neural network.</p>

<h2 id="122-model-neural-network-architecture">12.2 Model Neural Network Architecture</h2>
<p>Our first layer is our input layer and we will need to pass the shape of our input data to it. We will make use of <code class="highlighter-rouge">input_dim</code> to pass the dimensions of the input data to the <code class="highlighter-rouge">Dense</code> layer.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Import `Sequential` from `keras.models`</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="c"># Import `Dense` from `keras.layers`</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="c"># Initialize the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c"># Add input layer </span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    
<span class="c"># Add output layer </span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre>
</div>

<p>Our input layer needs to know the input dimension of the data, so we will pass in the input dimensions, which is 12. We also need to remember that we are counting the <code class="highlighter-rouge">Type</code> columns that were generated in our first model.</p>

<p>We will use the <code class="highlighter-rouge">relu</code> activation function, with no bias involved, and the number of hidden units will be <code class="highlighter-rouge">64</code>.</p>

<p>Our network ends with a single unit <code class="highlighter-rouge">Dense(1)</code>, and doesn’t include an activation. This is a typical setup for <code class="highlighter-rouge">scalar regression</code>, where we are trying to predict a single continuous value.</p>

<h2 id="123-compile-the-model-fit-the-data">12.3 Compile The Model, Fit The Data</h2>
<p>With our model at hand, we can now compile and fit the data to it with the <code class="highlighter-rouge">K</code> fold validation partitions:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Epoch 1/10
5195/5195 [==============================] - 0s - loss: 14.2885 - mean_absolute_error: 3.3583     
Epoch 2/10
5195/5195 [==============================] - 0s - loss: 1.6251 - mean_absolute_error: 0.9938     
Epoch 3/10
5195/5195 [==============================] - 0s - loss: 1.0007 - mean_absolute_error: 0.7737     
Epoch 4/10
5195/5195 [==============================] - 0s - loss: 0.7484 - mean_absolute_error: 0.6665     
Epoch 5/10
5195/5195 [==============================] - 0s - loss: 0.6334 - mean_absolute_error: 0.6156     
Epoch 6/10
5195/5195 [==============================] - 0s - loss: 0.5716 - mean_absolute_error: 0.5859     
Epoch 7/10
5195/5195 [==============================] - 0s - loss: 0.5392 - mean_absolute_error: 0.5709     
Epoch 8/10
5195/5195 [==============================] - 0s - loss: 0.5210 - mean_absolute_error: 0.5611     
Epoch 9/10
5195/5195 [==============================] - 0s - loss: 0.5098 - mean_absolute_error: 0.5565     
Epoch 10/10
5195/5195 [==============================] - 0s - loss: 0.5004 - mean_absolute_error: 0.5520     - ETA: 0s - loss: 0.4703 - mean_absolute_error: 0.
Epoch 1/10
5197/5197 [==============================] - 0s - loss: 12.4811 - mean_absolute_error: 3.0996     
Epoch 2/10
5197/5197 [==============================] - 0s - loss: 1.4883 - mean_absolute_error: 0.9524     
Epoch 3/10
5197/5197 [==============================] - 0s - loss: 0.9228 - mean_absolute_error: 0.7451     
Epoch 4/10
5197/5197 [==============================] - 0s - loss: 0.7208 - mean_absolute_error: 0.6578     
Epoch 5/10
5197/5197 [==============================] - 0s - loss: 0.6226 - mean_absolute_error: 0.6112     
Epoch 6/10
5197/5197 [==============================] - 0s - loss: 0.5700 - mean_absolute_error: 0.5861     
Epoch 7/10
5197/5197 [==============================] - 0s - loss: 0.5395 - mean_absolute_error: 0.5695     
Epoch 8/10
5197/5197 [==============================] - 0s - loss: 0.5199 - mean_absolute_error: 0.5597     
Epoch 9/10
5197/5197 [==============================] - 0s - loss: 0.5092 - mean_absolute_error: 0.5533     
Epoch 10/10
5197/5197 [==============================] - 0s - loss: 0.4996 - mean_absolute_error: 0.5487     
Epoch 1/10
5197/5197 [==============================] - 0s - loss: 13.3672 - mean_absolute_error: 3.2182     
Epoch 2/10
5197/5197 [==============================] - 0s - loss: 1.5880 - mean_absolute_error: 0.9708     
Epoch 3/10
5197/5197 [==============================] - 0s - loss: 0.9822 - mean_absolute_error: 0.7553     
Epoch 4/10
5197/5197 [==============================] - 0s - loss: 0.7341 - mean_absolute_error: 0.6579     
Epoch 5/10
5197/5197 [==============================] - 0s - loss: 0.6144 - mean_absolute_error: 0.6033     
Epoch 6/10
5197/5197 [==============================] - 0s - loss: 0.5527 - mean_absolute_error: 0.5755     
Epoch 7/10
5197/5197 [==============================] - 0s - loss: 0.5230 - mean_absolute_error: 0.5603     
Epoch 8/10
5197/5197 [==============================] - 0s - loss: 0.5081 - mean_absolute_error: 0.5551     
Epoch 9/10
5197/5197 [==============================] - 0s - loss: 0.4966 - mean_absolute_error: 0.5446     
Epoch 10/10
5197/5197 [==============================] - 0s - loss: 0.4881 - mean_absolute_error: 0.5421     
Epoch 1/10
5199/5199 [==============================] - 0s - loss: 12.9529 - mean_absolute_error: 3.1443     
Epoch 2/10
5199/5199 [==============================] - 0s - loss: 1.5940 - mean_absolute_error: 0.9716     
Epoch 3/10
5199/5199 [==============================] - 0s - loss: 1.0038 - mean_absolute_error: 0.7739     - ETA: 0s - loss: 1.0516 - mean_absolute_error: 0.
Epoch 4/10
5199/5199 [==============================] - 0s - loss: 0.7599 - mean_absolute_error: 0.6665     
Epoch 5/10
5199/5199 [==============================] - 0s - loss: 0.6336 - mean_absolute_error: 0.6078     
Epoch 6/10
5199/5199 [==============================] - 0s - loss: 0.5754 - mean_absolute_error: 0.5806     
Epoch 7/10
5199/5199 [==============================] - 0s - loss: 0.5403 - mean_absolute_error: 0.5648     
Epoch 8/10
5199/5199 [==============================] - 0s - loss: 0.5177 - mean_absolute_error: 0.5536     
Epoch 9/10
5199/5199 [==============================] - 0s - loss: 0.5049 - mean_absolute_error: 0.5502     
Epoch 10/10
5199/5199 [==============================] - 0s - loss: 0.4943 - mean_absolute_error: 0.5436     
Epoch 1/10
5200/5200 [==============================] - 0s - loss: 11.7594 - mean_absolute_error: 2.9585     
Epoch 2/10
5200/5200 [==============================] - 0s - loss: 1.6008 - mean_absolute_error: 0.9801     
Epoch 3/10
5200/5200 [==============================] - 0s - loss: 1.0165 - mean_absolute_error: 0.7773     
Epoch 4/10
5200/5200 [==============================] - 0s - loss: 0.7624 - mean_absolute_error: 0.6708     
Epoch 5/10
5200/5200 [==============================] - 0s - loss: 0.6438 - mean_absolute_error: 0.6162     
Epoch 6/10
5200/5200 [==============================] - 0s - loss: 0.5871 - mean_absolute_error: 0.5885     
Epoch 7/10
5200/5200 [==============================] - 0s - loss: 0.5504 - mean_absolute_error: 0.5729     
Epoch 8/10
5200/5200 [==============================] - 0s - loss: 0.5221 - mean_absolute_error: 0.5578     
Epoch 9/10
5200/5200 [==============================] - 0s - loss: 0.5104 - mean_absolute_error: 0.5518     
Epoch 10/10
5200/5200 [==============================] - 0s - loss: 0.4994 - mean_absolute_error: 0.5460     
</code></pre>
</div>

<p>We use the <code class="highlighter-rouge">compile()</code> function to compile the model and then the <code class="highlighter-rouge">fit()</code> function to fit the model to the data. When we compile the model we need to make sure that we define at minimum the <code class="highlighter-rouge">optimizer</code> and <code class="highlighter-rouge">loss</code> arguments. We then pass <code class="highlighter-rouge">rsmprop</code> to our optimizer, and <code class="highlighter-rouge">mse</code> to the loss function.</p>

<p>The additional <code class="highlighter-rouge">metrics</code> argument that we define is actually a function that is used to judge the performance of our model. For regression problems, it’s very common to take the <code class="highlighter-rouge">Mean Absolute Error (MAE)</code> as a metric. Finally, we determine how many epochs we want to run the fitting.</p>

<h2 id="124-evaluate-model">12.4 Evaluate Model</h2>
<p>We can evaluate our model by making use of the <code class="highlighter-rouge">Mean Squared Error (MSE)</code> and the <code class="highlighter-rouge">Mean Absolute Error (MAE)</code>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">mse_value</span><span class="p">,</span> <span class="n">mae_value</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Mean Squared Error:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mse_value</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">""Mean Absolute Error:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mae_value</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Mean Squared Error:
0.521800081686

Mean Absolute Error:
0.561619131769
</code></pre>
</div>

<p>Note that besides the <code class="highlighter-rouge">MSE</code> and <code class="highlighter-rouge">MAE</code> scores, we could also use the <code class="highlighter-rouge">R2 score</code> or the <code class="highlighter-rouge">regression score</code> function.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>

<span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span>
</code></pre>
</div>

<p>0.3125092543</p>

<p>Our model didn’t perform has well as we hoped, so we are going to experiment with optimizing the code so that the results become a little bit better.</p>

<h2 id="125-model-fine-tuning">12.5 Model Fine-Tuning</h2>
<p>Most of the time we will need to fine-tune our model because not all problems are as straightforward as we would like them to be.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<p><strong>Hidden Units</strong><br />
We can try adding more hidden units to our model’s architecture and study the effect on the evaluation:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<p>Something to keep in mind is that, because we don’t have a ton of data, we are more prone to overfitting. That’s why we should work with a small network.</p>

<h1 id="13-more-experiments-optimization-parameters">13. More Experiments: Optimization Parameters</h1>
<p>Besides adding layers and playing around with the hidden units, we can also try to adjust some of the parameters of the optimization algorithm that we give to the <code class="highlighter-rouge">compile()</code> function. We can try, for example, importing <code class="highlighter-rouge">RMSprop</code> from <code class="highlighter-rouge">keras.models</code> and adjust the learning rate <code class="highlighter-rouge">lr</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">RMSprop</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">rmsprop</span> <span class="o">=</span> <span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">rmsprop</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<p>Using the Stochastic Gradient Descent (SGD) optimization algorithm:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span><span class="p">,</span> <span class="n">RMSprop</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">sgd</span><span class="o">=</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

    </article>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="http://localhost:4000/assets/js/validator.js"></script>
<script src="/assets/js/app.js"></script>

    </section>
  </body>
</html>
